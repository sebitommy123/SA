
HIGH PRIO

I don't like the make_object API for SA, its ugly

I want to create an app that can run multiple providers
    As such, you should be able to define multiple providers from just one python file and pass them on
    multiple_providers will then just run them all in separate processes and create a UI dashboard for you to see how they're doing

Fix GroupObjects function, it is stupid and doesn't consider Type

I think all operators should instantly return AbsorbingNone if passed in as context (or arguments?)


Source selector doesn't work anymore, because it can't break up ObjectGroupings
    This dies: python -m sa ".map(@source_A.lol)"

This dies:
    python -m sa ".map(.salary == .manager)"

XML custom type

Lazy loading
and remember, .show_plan() should not just show have the SAP give instructions on how you could yourself query this stuff if you wanted
 * trades
    - by timestamp, max 2 seconds worth
 * network switches + interfaces + FPGAs
    - basically lazy loaded links, whenever you access it, it queries for more data
 * info decodes
    - by timestamp, similar to trades
 * exec decodes
    - by timestamp, similar to trades
 * pcaps
    - unclear
 * config-intent XML
    - unclear
 * history
    - basically a lazy-loaded link, but you have to specify a time range, which can only be so large depending on what it is
    - .history() -> which actually just lazy loads a field
 * extra host info (part of the object is lazy-loaded)
 * instruments
 * IBs
 * logs

I don't want to have to give people saps.txt files, should have a registry

comparing Link types with == is broken
    need a way of debugging links

lists of Objects need to be turned into ObjectLists

named contexts


MID PRIO

Foreach operator {}

Annotations for objects and fields.

aliases, including aliases that become whole queries and aliases that can take in arguments
e.g. map_port AA -> [.port == 'AA'] using a simple template

Takes too long to downlosd data from SAPs

sa-shell should store data so opening it is fast and it refreshes in background

test mode for SAPs so it returns output and doesn't start the server

AND and OR don't work yet bc of operator precedence not being a thing
 1. Should also add ()

 a way to list all providers and exposed sources


LOW PRIO

Context-less functions should not have a dot "." at the beginning. These functions feed context straight through to their arguments. E.g. equals() feels wrong to type with a "." for a good reason.

Better indexing, not hardcoded to types and Ids. Have to give it more thought.
    Needs to be able to be copied to new ObjectLists after filtering or selecting, etc...

autocomplete / suggest

an optimization: query caching, so if you run the same thing twice, it remembers the result. Or if you run A, it remembers it so A.B runs fast

notion of data "age"

concat object lists, "+" works

more operators: >< .any(), .counts() -> count of each thing in list

perhaps should be powered by Pandas for C acceleration?






--------
Some shits going down here
host#<application[.enabled == "Disabled"].host_name>
this creates a new query for each result of the previous query, and returns a new object list
which I think should be equivalent to
application[.enabled == "Disabled"].host_name.replace_each(host#<>)
but {} is kinda our syntax for replace each so
application[.enabled == "Disabled"].host_name{host#<>)
to preserve the app, you could do
application[.enabled == "Disabled"]{.host_name.replace(host#<>)}

application{APP => .host_name = *host[.name == APP.host_name]}
equivalent to
application{APP => APP.host_name = *host[.name == APP.host_name]}


EXAMPLE:
get apps with the same id as their hosts
app[host[.apps.includes("lol")].name == .name]

maybe instead of passing down "all_data", there should be a first-class concept of "context-stack"
    that is, every time a function mutates the context, it actually just gets added to the stack:
        when a function returns something in a chain, if there is something after it in the chain, the return value will be added to the context stack. all functions then just use the latest thing in the context stack by default.
        when a function executes a chain passed in as an argument, passing in custom context, it also just adds the custom context to the stack and passes in the stack.
    then, .pop_context(n) (special operator) can be used to pop a certain number of contexts during a chain. the shorthand is *, which calls .pop_context(-1), popping all the way back to all_data. you can then also do *[-1] which calls .pop_context(1), etc... 
        for example app[#host_01.name == .name]
            this won't work because #lol will execute on just the app, returning nothing.
            instead: app[*#lol.name == .name]
                this runs #lol on all_data
        for example app#mds_01.host.apps[.core == *[-2].sys_core]
            this will probably grab the host in *[-2]
        ofc this syntax is super ugly and basically useless, so probably I should only support a named context stack. i.e. we only keep the contexts you name
        for example app#mds_01.host=>HOST.apps[.core == HOST.sys_core]
            so this way, the context_stack just has "*" by default, and then other capitalized saved contexts
            the => operator is a special operator .add_context_to_stack("HOST") which adds to the stack
        for example app[host[HOST => .apps.includes(HOST.name)].name == .name]
            this is an example of naming the context in operators that craft new syntax (the filter operator literally picks what context what to pass into the arg operator, but will also pass the context_stack with the extra named argument if you choose to)
        NEW IDEA
            operators now only take in context_stack (and arguments ofc), no context, no all_data
            then the context_stack always has:
                all_data (returns the original complete object list)
                current_context (returns the latest object list, likely created by the previous operator in the chain)
                named_contexts (a dict of name => context)
                ONLY named_contexts is editable (add-only) via a .add_named_context() method. everything else is read-only
            what operators return always becomes the current_context if in a chain
                the return is usually just used as the context by the next operator
                sometimes if an operator is directly running this other operator (e.g. filter), it can use that return value directly
            then, there's the .add_named_context(name) operator
                which just adds to named_contexts the current_context with the name
            and there's the .get_named_context(name) operator
                which just returns the named context! which ofc then becomes the current_context if in a chain :)
        
            
        
            

-------
SELECT operator overhaul
which can also be a MAP now

 1. Grab a set of fields from all objects, keep only those alongside id/type/source still there
 2. Replace each object with the result of a expression ran on the object
 3. Replace each object's attribute with the result of a expression

SELECT mode:
    app[["name", "host"]]

FOREACH mode:
    app{.name = "lol", .neighbors = .host.apps.count()}

MAP mode:
    app.map(.name)

Supports named context ofc
application{APP => APP.host_name = *host[.name == APP.host_name]}






----------------------
XML
#sphr_exec_sessions.children[.name == "Sessions"][0].children[.name == "Market"][0].value
#sphr_exec_sessions.children[.name == "Sessions"][0].children[.name == "MiaxPurgeIds"][0].children[.name == "MiaxPurgeId"][[.attr_name, .attr_purgeSessionId]]

all children's names exposed as a link to it, will be omitted if there's multiple
all attributes also exposed as links, higher presedence to children names
you can use the [] syntax of XML node objects and it will implicitly call .children

.Sessions.Market
	-> .children[.name == "Sessions"][0].children[.name == "Market"][0]

.Sessions.MiaxPurgeIds.MiaxPurgeId
	-> Doesn't work bc multiple MiaxPurgeId

.Sessions.MiaxPurgeIds[[.name, .purgeSessionId]]


.Sessions.Market

.Sessions.Market
.Sessions.MiaxPurgeIds.MiaxPurgeId[0]


.ProductUnderlyingMappingsConfiguration.ProductUnderlyingMappings[.underlying == "TFC"][0].products

need a shorthand for [.underlying == "TFC"], it's still too long. maybe I should just be able to omit the ""

Also I do still believe that the get_field "." operator should honestly be able to deal with lists that have just 1 element in it by automatically unwrapping them.

That way it becomes:
#product_underlying_mappings.ProductUnderlyingMappingsConfiguration.ProductUnderlyingMappings[.underlying == TFC].products

Also these files should be lazy loaded, which requires solving figuring out which SAP to hit given an id


-----

lazy-loading

lazy_query("trade", date=='YYYYMMDD', cp=='JS')
lazy_query("host", id=="host_001")
lazy_query("interface", host=="host_001")

lazy_query will:
 1. Figure out all the SAPs that lazily serve this type
 2. Query them with the filter conditions
 3. Report if any of them deny on lack of specificity
 4. Add all the results to client-side all_data, as well as add them to the context
 5. Cache the query for the amount of time the server specifies (next time this query is called, if in that TTL, it will just instantly return the same thing as before)

There's also lazy_query_plan, which just has the SAP return what it will do (or deny)

It has to be a runtime thing, bc at static time we don't have enough information to plan out the lazy_queries.

Two main cases:
 1. Query by type and some other fields to get an ObjectList (e.g. trades, interfaces)
 2. Query by type and id to get a single SAObject

Some situations:
 1. You don't have the type but have an ID, in which case you need SAPs to tell you likely types
 2. You have a link which then has a filter applied, so clearly have to not lazy load until the last moment


Issues:
 1. With static analysis you know what will happen in the future
 2. With runtime state you have access to runtime values

We need a mix, essentially we do a static analysis of each chain before running it.

Scopes have:
 1. A list of conditions, each has a field on the left, an operator and a constant on the right. The field can't be a link.

We need to make a distinction between passive and active operators:
 1. Passive operators do not modify the data
 2. Passive operators can remove certain Objects or fields from the objects
 3. Passive operators can be ran in theoretical analysis mode, where they return a scope that they would be left with. A Scope has a) list of conditions b) fields of interest

SAPs advertise:
 1. A list of scopes (including fields) that it has. If it doesn't know fields because it depends on the query, it just says "*"

Requirements:
 1. For this to work we have to fix the "hidden query" issue created by links.
    e.g. #app_01.trades[.date == 'YYYYMMDD']
    The link also acts as a "restart" point for static analysis.
    It actually works because get_field is already a passive operator, in this case it just gets resets the scope so far!
 2. Also we have to fix repetitive queries in filter operators that could be replaced by single larger queries.
    e.g. app[.trades[.date == 'YYYYMMDD'].count() > 1]
    In this case, we really want to end up sending a query for all apps that we will process.
    Easy solution! Before running a loop with [] or {}, calculate all of the scopes for each one

System:
 1. Before executing each chain, statically extract the scope until we hit a non-passive operator.


Unknown attribute
 1. An unknown_attribute is a (type, id, attribute) for which a SAP says they have information but is has not been downloaded

Static analysis:
 1. Repeat these steps
 1. You have a list of known_scopes before starting, starts at []
 2. As well as a list of needed_scopes
 3. For each passive operator, it determines whether or not all the attributes

How to execute a query:
 1. Queries now output the scopes they need to execute. They do this by having each operator output not just the result, but also the scopes they need. This is called scope_inspect on an operator. Scope_inspect takes in the scopes that are currently needed (starting with all of them), and can modify them. They can also move scopes or just add new scopes into the staged area, where they can no longer be modified. Examples:
    .filter_by_type removes any of the current scopes that aren't of this type
    .get_field just modifies the current scope to only need the field. if it's a link, it actually stages all current scopes, and then executes the link and sets the returning scopes to the new current scopes
    .set_field adds a staged scope which is just the current scopes with just the field needed
    .filter_by_source will just get rid of the scopes without this source
 2. Then, when we're done, we check if all the scopes (staged and current) are KNOWN, meaning data for them is downloaded. If it isn't we download it and restart from step (1)! Re-execute with more data. If it's known, we're done and return the result.
This system allows the resolving of lazy-loaded recursive links.
This also means that, alongside all_data, we need to pass in all_scopes. Perhaps scopes can just be part of ObjectList for simplicity. Maybe ObjectList should just be called QueryExecutionState.
Also, we explicitly are fine with errors, if there's still some scopes to download. The errors could be because of the missing scope!

app#onxt_001.trades[.date == 'YYYYMMDD']
.filter_by_type("app").get_by_id("onxt_001").get_field("trades").filter(.equals(.get_field("date"), "YYYYMMDD"))


There are two kinds of lazy-loading, exhaustive and non-exhaustive:
 1. For exhaustive, we know the full list of (type, id) objects beforehand, and there's not that many. (<1M)
        The SAP can mark certain types as exhaustive lazy-loading in /hello
 2. For non-exhaustive, we don't know ahead of time.

If the shell knows it has all of them, it can then limit any queries to just the objects it actually cares about.

Problem: If just one employee is left, but not because you filtered by id, then there should be an implicit id filter added.

OR just give a list of IDs when querying, SAP can choose to use it


Problem: How do I keep track of what I've downloaded?
 1. I can cache exact queries, so I don't send them again.
 2. The issue is that queries often include many extra conditions that aren't necessary.
 3. How can we know which ones the server even cares about? Maybe they can just tell us.
 4. For each scope it would tell you the Filtering_Fields and also whether or not it cares about id_types.

id_types is fundamentally flawed
 1. It needs to get staged too!
 2. At this point it should work like this:

Problem: we can't tell the difference between a real runtime error and just not having enough data. We need to be able to tell the difference. New errors:
  QueryError -> Has a "could_succeed_with_more_data" option
    - e.g. [0] index errors always populate this with "true"
    - e.g. get_field also populates this
  We always try to get more data (it's required for successful execution).
  But if we get an error without could_succeed_with_more_data=True, then we just give up and show the error.


racks#some_rack.right_neighbor.right_neighbor.right_neighbor
.filter_by_type("racks").get_by_id("some_rack").get_field("right_neighbor").get_field("right_neighbor").get_field("right_neighbor")


Available: rack#some_rack.right_neighbor, rack#rack_002.right_neighbor
Staged: [Racks(id='some_rack', fields="right_neighbor"), Racks(id="rack_002", fields="right_neighbor")]
Current: []


ALL_SCOPES [Trades(fields="*", conditions=[], sap='tt'), Irrelevant(), Apps]

SCOPES [App(id="onxt_001", fields="trades"), Trades(id="onxt_001", date='YYYYMMDD')]

NEEDED_SCOPES=


apps[.host.name == .nvs.host.name]

trades[.cp=="JS"]

#host_001

#host_001.interfaces[0].switch.interfaces[4].switch

#app_001.trades[.date == 'YYYYMMDD']

Each SAP advertises on /hello:
 1. A list of Lazy Loaded types it supports. Clients can send LazyQueries for a type to SAPs and they will be executed or declined.
 2. A list of type_ids that it supports, to help with infering type when just an id is available.

ObjectList has a list lazy_loaded_object_lists which is a list of LazyLoadedObjectList
    A LazyLoadedObjectList has a Type, 


-----

Executing a query

An operator takes in arguments and optionally a context.
The special sauce is that there is an all-pervasive context, that passes through most operators, allowing compact syntax.

Now, the ObjectList should be:
 1. StagedObjectLists (the ObjectList stack)
 2. NeededScopes
 3. StagedScopes

Any query can modify the ObjectList
They still take in the ObjectList as context separately.
